## ***Efficiently Learning an Encoder that Classifies Token Re- placements Accurately (ELECTRA)***:snowflake:


## 1. Introduction
   - Transformer based architecture has tremendous potential and usage in NLP related tasks.The advantage of this method is that you only need labelled data for the   final step (fine-tuning). 
   - Pre-train the Transformer on the huge dataset, and fine-tune the pre-trained Transformer on your particular task, using your task-specific dataset (which may be comparatively tiny).
  
   ***One Such Methods is BERT.***
  
  - Well, the original BERT model relied on two pre-training tasks, **masked language modelling (MLM) and next sentence prediction**. 
  - In next sentence prediction, the model is tasked with predicting whether two sequences of text naturally follow each other or not. This task was said to help with certain downstream tasks such as Question Answering and Natural Language Inference in the BERT paper although it was shown to be unnecessary in the later RoBERTa paper which only used masked language modelling. 
  - Regardless, we are more interested in the first approach, MLM, as this is what the ELECTRA pre-training method aims to improve upon.
  
  
 ## 2. Pre-Training
 
  -  Pre-training is the process through which the Transformer model learns to model a language. In other words, the Transformer will learn good, context-dependent ways of representing text sequences.
  
  ***What is Electra? How it performs pre-training? ***
   
 - Electra is based on the idea of **replaced token detection**.
 - a pre-training task in which the model learns to distinguish real input tokens from plausible but synthetically generated replacements. 
 - Instead of masking, it corrupts the input by replacing some tokens with samples from a proposal distribution, which is typically the output of a small masked language model.
 - ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) is a new pre-training approach which aims to match or exceed the downstream performance of an MLM pre-trained model while using significantly less compute resources for the pre-training stage. 
 - The pre-training task in ELECTRA is based on detecting replaced tokens in the input sequence. This setup requires two Transformer models, a generator and a discriminator.
 - ![image](https://github.com/99sbr/NLP-Research-Papers/blob/master/Screenshot%202020-08-31%20at%2020.41.52.png)
 
## 3. Deep Dive into Approach

  -  In papaer they have discussed the approach to trains two neural networks: -
       - a generator G and a discriminator D. 
       - Each one primarily consists of an encoder (e.g., a Transformer network) 
  - **train the generator with maximum likelihood** rather than adversarially due to the difficulty of applying GANs to text.
  - In adversarial training, we seek a generative model that can produce (by sampling) data that is indistinguishable from the data used for training. When dealing with textual data it becomes quiet challeging.
  
  - ***Why not GANs for text?***
       There is a problem in the combination of how GANs work and how text is normally generated by neural networks:
      * GANs work by propagating gradients through the composition of Generator and Discriminator.
      * Text is normally generated by having a final softmax layer over the token space, that is, the output of the network is normally the probabilities of    generating   each token (i.e. a discrete stochastic unit).
      * These 2 things do not work well together on their own, because you cannot propagate gradients through discrete stochastic units. 
  - Input: tokens **x = [x1, ..., xn]** into a sequence of contextualized vector representations **h(x) = [h1, ..., hn]**. 
  - For a given position t, (in our case only positions where x[t] = [MASK]), the generator outputs a probability for generating a particular token x[t] with a softmax layer and discriminator predicts wheather it is real or fake.
  - ***Mentioned in paper** The Generator is trained with Maximum likelihood rather than adverserial. Adverserially training Generator is challenging because it is impossible to backpropagate through genertor through sampling steps.
  - discriminator model is trained to predict which tokens have been replaced given a corrupted sequence. 
  - *This means that the discriminator loss can be calculated over all input tokens as it performs prediction on each token. With MLM, the model loss is only calculated over the masked tokens. This is shown to be a key difference between the two approaches and the primary reason behind ELECTRAâ€™s greater efficiency.*
  - 
  
## 4. Efficiency Gains of ELECTRA approach
  - ***Loss Defined over all input tokens VS only masked tokens***
  - The discriminator model's loss is calculated over all input tokens in the sequence.
  - To demonstrate the significance of this difference, the authors compare the ELECTRA model to an identically trained model (ELECTRA 15%) except that ELECTRA 15% only calculates the discriminator loss over the masked tokens. The GLUE benchmark is used for the comparison.
  - ![Image](https://github.com/99sbr/NLP-Research-Papers/blob/master/Screenshot%202020-09-13%20at%2020.29.49.png)
  - ![Image](https://github.com/99sbr/NLP-Research-Papers/blob/master/Screenshot%202020-09-13%20at%2020.30.08.png)
  
 ## 5. Model Extensions
   - ***Weights Sharing:***
   - While pre-training weights are shared between Generator and Discriminator given they are of same size.
   - However it is more efficient to use small generators and share only the embeddings with discriminator.
   - This strategy benefits ELECTRA in way that MLM excels in learning the word representations and discriminator can directly use the embeddings over softmax to do the corrupt classification. 
   
   - ***Small Generators***
   - Small generators helps to speed up training process.
   - Too strong generators can pose a problem to discriminators, as discriminator may use most of its parameters modelling generator rather than actual learning.
   -
